{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-06T14:35:01.374386Z",
     "start_time": "2024-10-06T14:35:01.369165Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "from datasets import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.decomposition import IncrementalPCA, PCA\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "from incidents_dataset import load_compressed_images"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-06T14:35:03.072275Z",
     "start_time": "2024-10-06T14:35:03.069236Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def batch_generator(batch_size):\n",
    "    generator = load_compressed_images('cached_train.bin')\n",
    "    \n",
    "    buffer = []\n",
    "    labels = []\n",
    "    for image, label in generator:\n",
    "        if len(buffer) < batch_size:\n",
    "            buffer.append(image.flatten())\n",
    "            labels.append(label.flatten())\n",
    "        else:\n",
    "            yield np.stack(buffer), np.stack(labels)\n",
    "            buffer.clear()\n",
    "            labels.clear()"
   ],
   "id": "3595b72a7a6bcb24",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-06T14:41:00.946586Z",
     "start_time": "2024-10-06T14:40:52.336378Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 2. Incremental PCA 설정\n",
    "n_components = 2  # 축소할 차원 수\n",
    "ipca = IncrementalPCA(n_components=n_components)\n",
    "\n",
    "batch_size = 256\n",
    "\n",
    "# 3. Generator로부터 데이터를 배치 단위로 읽어와 Incremental PCA 학습\n",
    "generator = batch_generator(batch_size)\n",
    "\n",
    "# partial_fit: Incremental PCA의 주성분을 점진적으로 학습\n",
    "for (batch, labels) in tqdm(generator, ncols=75):\n",
    "    if batch.shape[0] > 1:  # 배치 크기가 1보다 큰 경우만 처리\n",
    "        ipca.partial_fit(batch)\n",
    "        \n",
    "# 4. 학습된 Incremental PCA를 사용하여 데이터 차원 축소\n",
    "# 다시 generator를 사용하여 변환\n",
    "transformed_data = []\n",
    "\n",
    "generator = batch_generator(batch_size)\n",
    "for (batch, labels) in tqdm(generator, ncols=75):\n",
    "    if batch.shape[0] > 1:  # 배치 크기가 1보다 큰 경우만 처리\n",
    "        transformed_batch = ipca.transform(batch)\n",
    "        transformed_data.append(transformed_batch)\n",
    "        \n",
    "final_data = np.vstack(transformed_data)  # 모든 배치를 모아서 최종 결과 생성\n",
    "print(f\"Reduced data shape: {final_data.shape}\")"
   ],
   "id": "88bffebec0521e20",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0it [00:00, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "acb82323754b49878f77f1d067f6da43"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[11], line 13\u001B[0m\n\u001B[1;32m     11\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m (batch, labels) \u001B[38;5;129;01min\u001B[39;00m tqdm(generator, ncols\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m75\u001B[39m):\n\u001B[1;32m     12\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m batch\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:  \u001B[38;5;66;03m# 배치 크기가 1보다 큰 경우만 처리\u001B[39;00m\n\u001B[0;32m---> 13\u001B[0m         \u001B[43mipca\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpartial_fit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;66;03m# 4. 학습된 Incremental PCA를 사용하여 데이터 차원 축소\u001B[39;00m\n\u001B[1;32m     16\u001B[0m \u001B[38;5;66;03m# 다시 generator를 사용하여 변환\u001B[39;00m\n\u001B[1;32m     17\u001B[0m transformed_data \u001B[38;5;241m=\u001B[39m []\n",
      "File \u001B[0;32m~/Develop/Python/DiasterDetectionCore/.venv/lib/python3.10/site-packages/sklearn/base.py:1473\u001B[0m, in \u001B[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[0;34m(estimator, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1466\u001B[0m     estimator\u001B[38;5;241m.\u001B[39m_validate_params()\n\u001B[1;32m   1468\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[1;32m   1469\u001B[0m     skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[1;32m   1470\u001B[0m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[1;32m   1471\u001B[0m     )\n\u001B[1;32m   1472\u001B[0m ):\n\u001B[0;32m-> 1473\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfit_method\u001B[49m\u001B[43m(\u001B[49m\u001B[43mestimator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Develop/Python/DiasterDetectionCore/.venv/lib/python3.10/site-packages/sklearn/decomposition/_incremental_pca.py:356\u001B[0m, in \u001B[0;36mIncrementalPCA.partial_fit\u001B[0;34m(self, X, y, check_input)\u001B[0m\n\u001B[1;32m    345\u001B[0m     mean_correction \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39msqrt(\n\u001B[1;32m    346\u001B[0m         (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_samples_seen_ \u001B[38;5;241m/\u001B[39m n_total_samples) \u001B[38;5;241m*\u001B[39m n_samples\n\u001B[1;32m    347\u001B[0m     ) \u001B[38;5;241m*\u001B[39m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmean_ \u001B[38;5;241m-\u001B[39m col_batch_mean)\n\u001B[1;32m    348\u001B[0m     X \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mvstack(\n\u001B[1;32m    349\u001B[0m         (\n\u001B[1;32m    350\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msingular_values_\u001B[38;5;241m.\u001B[39mreshape((\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m1\u001B[39m)) \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcomponents_,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    353\u001B[0m         )\n\u001B[1;32m    354\u001B[0m     )\n\u001B[0;32m--> 356\u001B[0m U, S, Vt \u001B[38;5;241m=\u001B[39m \u001B[43mlinalg\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msvd\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfull_matrices\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcheck_finite\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m    357\u001B[0m U, Vt \u001B[38;5;241m=\u001B[39m svd_flip(U, Vt, u_based_decision\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m    358\u001B[0m explained_variance \u001B[38;5;241m=\u001B[39m S\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m2\u001B[39m \u001B[38;5;241m/\u001B[39m (n_total_samples \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m)\n",
      "File \u001B[0;32m~/Develop/Python/DiasterDetectionCore/.venv/lib/python3.10/site-packages/scipy/linalg/_decomp_svd.py:160\u001B[0m, in \u001B[0;36msvd\u001B[0;34m(a, full_matrices, compute_uv, overwrite_a, check_finite, lapack_driver)\u001B[0m\n\u001B[1;32m    156\u001B[0m lwork \u001B[38;5;241m=\u001B[39m _compute_lwork(gesXd_lwork, a1\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m], a1\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m1\u001B[39m],\n\u001B[1;32m    157\u001B[0m                        compute_uv\u001B[38;5;241m=\u001B[39mcompute_uv, full_matrices\u001B[38;5;241m=\u001B[39mfull_matrices)\n\u001B[1;32m    159\u001B[0m \u001B[38;5;66;03m# perform decomposition\u001B[39;00m\n\u001B[0;32m--> 160\u001B[0m u, s, v, info \u001B[38;5;241m=\u001B[39m \u001B[43mgesXd\u001B[49m\u001B[43m(\u001B[49m\u001B[43ma1\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcompute_uv\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcompute_uv\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlwork\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlwork\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    161\u001B[0m \u001B[43m                      \u001B[49m\u001B[43mfull_matrices\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfull_matrices\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moverwrite_a\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moverwrite_a\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    163\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m info \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m    164\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m LinAlgError(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSVD did not converge\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-06T14:12:04.973708Z",
     "start_time": "2024-10-06T14:12:04.925835Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# final_data는 차원이 축소된 데이터라고 가정\n",
    "# n_components = 2로 설정하여 시각화 용도로 다시 축소\n",
    "# pca_vis = PCA(n_components=2)\n",
    "# data_2d = pca_vis.fit_transform(final_data)\n",
    "\n",
    "# Isolation Forest를 사용하여 이상치 탐지\n",
    "iso_forest = IsolationForest(contamination=0.05, random_state=42)  # contamination: 이상치 비율\n",
    "outlier_labels = iso_forest.fit_predict(final_data)\n",
    "\n",
    "# 정상 데이터와 이상치를 분리\n",
    "normal_data = final_data[outlier_labels == 1]\n",
    "outlier_data = final_data[outlier_labels == -1]\n",
    "\n",
    "# 시각화\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.scatter(normal_data[:, 0], normal_data[:, 1], c='blue', label='Normal')\n",
    "plt.scatter(outlier_data[:, 0], outlier_data[:, 1], c='red', label='Outliers')\n",
    "plt.title('PCA Visualization of Reduced Data with Outliers')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 이상치 제거 후의 데이터\n",
    "cleaned_data = final_data[outlier_labels == 1]\n",
    "print(f\"Original shape: {final_data.shape}\")\n",
    "print(f\"Cleaned data shape: {cleaned_data.shape}\")"
   ],
   "id": "3955cbe622f1426d",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'final_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[7], line 4\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# final_data는 차원이 축소된 데이터라고 가정\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m# n_components = 2로 설정하여 시각화 용도로 다시 축소\u001B[39;00m\n\u001B[1;32m      3\u001B[0m pca_vis \u001B[38;5;241m=\u001B[39m PCA(n_components\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m)\n\u001B[0;32m----> 4\u001B[0m data_2d \u001B[38;5;241m=\u001B[39m pca_vis\u001B[38;5;241m.\u001B[39mfit_transform(\u001B[43mfinal_data\u001B[49m)\n\u001B[1;32m      6\u001B[0m \u001B[38;5;66;03m# Isolation Forest를 사용하여 이상치 탐지\u001B[39;00m\n\u001B[1;32m      7\u001B[0m iso_forest \u001B[38;5;241m=\u001B[39m IsolationForest(contamination\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.05\u001B[39m, random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m42\u001B[39m)  \u001B[38;5;66;03m# contamination: 이상치 비율\u001B[39;00m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'final_data' is not defined"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "4c3eaa08f83a3db8"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
